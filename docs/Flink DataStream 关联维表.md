>关联维表的三种基础方式：
>
>- 实时数据库查询关联 [ 流批关联 ]
>    - 同步数据库查询关联
>    - 异步数据库查询关联
>    - 带缓存的数据库查询关联
>- 预加载维表关联 [ 流批关联 ]
>    - 启动时预加载维表(每个并行全量加载)
>    - 启动时预加载分区维表(每个并行各加载部分)
>- 维表变更日志关联 [ 流流关联 ]

## 实时数据库查询关联

在DataStream API用户自定义Function中直接访问数据库进行关联的方式

优点：

- 开发量小
- 每一条数据都需要请求数据库，数据库压力大
- 关联时基于Processing Time，如果数据有延迟或者重复，会得到与原先不一致的数据

### 同步数据库查询关联

在一个Map Function或者FlatMap Function中访问数据库，处理好关联逻辑后，将结果数据输出

缺点：

- subtask线程会被阻塞，影响吞吐量

应用场景：用于流量低的实时计算 [ 通常不是最好的选择 ]

### 异步数据库查询关联

通过AsyncIO来访问外部数据库的方式。利用数据库提供的异步客户端，AsyncIO可以并发地处理多个请求，很大程度上减少了subtask线程的阻塞。

与同步数据库查询相比：

- 优点：提高了吞吐量

- 缺点：AsyncIO提供了有序和无序两种模式，有序模式下，需要将结果数据保存到checkpoint的状态中，存在一定的内存占用

应用场景：适用于流量低的实时计算

### 带缓存的数据库查询关联

引入缓存可以减少对数据库的请求，减轻数据库压力。

缓存可以使用 WeakHashMap 或者 Guava Cache 实现。 [ 没必要使用Redis或者Ehcache做数据库缓存，相对于Flink作业而言，太重了 ]

相比前面两种关联方式

优点：

- 增加了缓存，较少了数据库压力

缺点：
- 作业刚启动时，仍会对数据库造成一定压力
- 维表的更新，缓存无法及时更新

应用场景：适用于流量低，且维表数据实时性要求不高或者维表更新较少的业务场景

## 预加载维表关联

预加载维表：在作业启动时，就将维表读取到内存中，而在后续运行期间，每条数据都会和内存中的维表进行关联，而不会直接访问数据库

与带缓存的数据库查询关联相比：

- 带缓存的数据库查询关联：如果不命中缓存，还可以fallback到数据库访问
- 预加载维表关联：不命中，就是关联不到数据

### 启动时预加载维表

在作业初始化的时候，直接从数据库将维表内容拷贝到内存中。

优点：

- 运行期间，不再访问数据库，基于内存，效率更高

缺点：

- 在作业启动时期，要拷贝整个维表，短时间内，数据库压力会比较大
- 作业运行期间，维表数据无法更新
- 对TaskManager的内存要求比较高（每一个SubTask，都有一份全量的维表数据）

应用场景：适用于维表数据较小，且变更实时要求不高或者维表更新较少的业务场景 [ 维表发生变更时，重启作业即可 ]

### 启动时预加载分区维表

在预加载维表的基础上新增分区功能。简言之，就是每个SubTask，都只加载分区范围内的维表数据。

> 该分区方式，并不是根据keyby这种通用的hash分区，而是根据业务数据定制化分区策略。

相比全量的预加载维表

优点：

- 大大减少了对TaskManager的内存要求（每一个SubTask只有1/N份数据。）

应用场景：适用于维表数据较大，且变更实时要求不高或者维表更新较少的业务场景

### 启动时预加载维表并定时刷新

预加载维表时，除了维表大小的限制，还有一个维表数据更新的问题。可以引入定时刷新的机制来缓解这个问题。

定时刷新可以通过 Flink 的 ProcessFuntion 提供的Timer 或者 直接在 `open() ` 时初始化一个线程来做这个事。

缺点：

- 每一次维表更新，都是一次数据库的请求高峰，数据库压力变大

应用场景：满足了有维表实时变更要求，但是不是特别高的场景

### 启动时预加载维表 + 实时数据库查询

预加载维表与实时数据库查询的混用，将预加载的维表数据作为缓存，提供实时关联使用，若未命中，则fallback到数据库进行查找。

应用场景：适合流量低，且维表更新较少的业务场景

## 维表变更日志关联

维表变更日志关联，将维表以changelog数据流的方式表示，将维表关联转变为两个数据流的join。该方式，通常需要维表数据库端以push的方式将变更日志推送的消息队列中。

### Processing Time 维表变更日志关联

如果基于 Processing Time 做关联，我们可以利用 keyby 将两个数据流中关联字段值相同的数据划分到 KeyedCoProcessFunction 的同一个分区，然后用 ValueState 或者 MapState 将维表数据保存下来。在普通数据流的一条记录进到函数时，到 State 中查找有无符合条件的 join 对象，若有则关联输出结果，若无则根据 join 的类型决定是直接丢弃还是与空值关联。这里要注意的是，State 的大小要尽量控制好。首先是只保存每个 key 最新的维度数据值，其次是要给 State 设置好 TTL，让 Flink 可以自动清理。

优点：

- 不需要直接请求数据库，不会对数据库造成压力

缺点：

- 比较复杂，相当于使用 changelog 在 Flink 应用端重新构建一个维表，会占用一定的 CPU 和比较多的内存和磁盘资源

> 可以利用 Flink 提供的 RocksDB StateBackend，将大部分的维表数据存在磁盘而不是内存中，所以并不会占用很高的内存
>
> 基于 Processing Time 的这种关联对两个数据流的延迟要求比较高，否则如果其中一个数据流出现 lag 时，关联得到的结果可能并不是我们想要的，比如可能会关联到未来时间点的维表数据。

应用场景：适用于不便直接访问数据的场景（比如维表数据库是业务线上数据库，出于安全和负载的原因不能直接访问），或者对维表的变更实时性要求比较高的场景（但因为数据准确性的关系，一般用 Event Time 关联会更好）。

### Event Time 维表变更日志关联

将维表 changelog 的多个时间版本都记录下来，然后每当一条记录进来，我们会找到对应时间版本的维表数据来和它关联，而不是总用最新版本，因此延迟数据的关联准确性大大提高

> 目前 State 并没有提供 Event Time 的 TTL，因此我们需要自己设计和实现 State 的清理策略

相比基于 Processing Time 的维表变更日志关联

优点：

- 数据的准确性更高
- 两个数据流的延迟要求低

缺点：

- 因为存了多个时间版本的维表数据，导致空间资源消耗更大

应用场景：维表变更比较多且对变更实时性要求较高的场景 同时也适合于不便直接访问数据库的场景

### Temporal Table Join

Temporal Table Join 是 Flink SQL/Table API 的原生支持，它对两个数据流的输入都进行了缓存，因此比起上述的基于 Event Time 的维表变更日志关联，它可以容忍任意数据流的延迟，数据准确性更好。Temporal Table Join 在 SQL/Table API 使用时是十分简单的，但如果想在 DataStream API 中使用，则需要自己实现对应的逻辑。

总体思路是使用一个 CoProcessFunction，将 build 数据流以时间版本为 key 保存在 MapState 中（与基于 Event Time 的维表变更日志关联相同），再将 probe 数据流和输出结果也用 State 缓存起来（同样以 Event Time 为 key），一直等到 Watermark 提升到它们对应的 Event Time，才把结果输出和将两个数据流的输入清理掉。

 Watermark 触发很自然地是用 Event Time Timer 来实现，但要注意不要为每条数据都设置一遍 Timer，因为一旦 Watermark 提升会触发很多个 Timer 导致性能急剧下降。比较好的实践是为每个 key 只注册一个 Timer。实现上可以记录当前未处理的最早一个 Event Time，并用来注册 Timer。当前 Watermark。每当 Watermark 触发 Timer 时，我们检查处理掉未处理的最早 Event Time 到当前 Event Time 的所有数据，并将未处理的最早 Event Time 更新为当前时间。

优点：

- 对于两边数据流的延迟的容忍度较大

缺点：

- 会有一定的输出延迟（这也是基于 Watermark 机制的计算的常见问题）
- Flink应用的空间资源要求大很多

> 如果维表变更太慢，导致 Watermark 提升太慢，会导致 probe 数据流被大量缓存，所以最好要确保 build 数据流尽量实时，同时给 Source 设置一个比较短的 idle timeout。

应用场景：数据准确性最好，适合一些对数据准确性要求高且可以容忍一定延迟（一般分钟级别）的关键业务。

## 参考

1. Flink 中文社区：https://flink-learning.org.cn/article/detail/b8df32fbc6542257a5b449114e137cc3?spm=a2csy.flink.0.0.49493bdcLZAWkq
